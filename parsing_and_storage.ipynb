{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Parsing",
   "id": "529e712c7fd45076"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T19:10:15.963130Z",
     "start_time": "2025-03-03T19:08:52.017489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time  # To avoid excessive requests\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "# Wikipedia URL\n",
    "BASE_URL = \"https://en.wikipedia.org\"\n",
    "LIST_URL = BASE_URL + \"/wiki/List_of_highest-grossing_films\"\n",
    "\n",
    "# Database setup\n",
    "def setup_database():\n",
    "    conn = sqlite3.connect(\"films.db\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Drop the table if it exists to avoid duplicates\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS films\")\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS films (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            rank TEXT,\n",
    "            peak TEXT,\n",
    "            title TEXT NOT NULL,\n",
    "            box_office_revenue TEXT,\n",
    "            release_year INTEGER,\n",
    "            country TEXT,\n",
    "            director TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    return conn, cursor\n",
    "\n",
    "# Clean text with joined words and remove duplicates\n",
    "def clean_joined_words(text):\n",
    "    # Insert a comma and space before a capital letter if it's preceded by a lowercase letter\n",
    "    # This handles cases like \"United KingdomUnited States\" -> \"United Kingdom, United States\"\n",
    "    cleaned_text = re.sub(r'([a-z])([A-Z])', r'\\1, \\2', text)\n",
    "    \n",
    "    # Replace newlines with commas\n",
    "    cleaned_text = re.sub(r'\\n', ', ', cleaned_text)\n",
    "    \n",
    "    # Clean up any double commas or spaces\n",
    "    cleaned_text = re.sub(r',\\s*,', ',', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    # Trim whitespace\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    \n",
    "    # Remove duplicates by splitting, creating a unique set, and rejoining\n",
    "    if ',' in cleaned_text:\n",
    "        items = [item.strip() for item in cleaned_text.split(',')]\n",
    "        # Create a list with unique items while preserving order\n",
    "        unique_items = []\n",
    "        for item in items:\n",
    "            if item and item not in unique_items:\n",
    "                unique_items.append(item)\n",
    "        cleaned_text = ', '.join(unique_items)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Extract plainlist content properly\n",
    "def extract_plainlist_content(element):\n",
    "    if not element:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Check if there's a plainlist with <ul> and <li> elements\n",
    "    plainlist = element.find('div', class_='plainlist')\n",
    "    if plainlist:\n",
    "        lis = plainlist.find_all('li')\n",
    "        if lis:\n",
    "            # Join all <li> elements with proper separator\n",
    "            return ', '.join(li.get_text(strip=True) for li in lis)\n",
    "    \n",
    "    # If no plainlist or no <li> elements found, try alternative approaches\n",
    "    links = element.find_all('a')\n",
    "    if links:\n",
    "        return ', '.join(link.get_text(strip=True) for link in links)\n",
    "    \n",
    "    # Just get all text with commas if other methods fail\n",
    "    text = element.get_text(separator=', ', strip=True)\n",
    "    \n",
    "    # Clean up the text to handle joined words\n",
    "    return clean_joined_words(text)\n",
    "\n",
    "# Fetch film data from Wikipedia\n",
    "def fetch_film_data():\n",
    "    # Fetch the page\n",
    "    response = requests.get(LIST_URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Locate the table (first table with class 'wikitable')\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "    # Extract film data\n",
    "    films = []\n",
    "    for row in tqdm(table.find_all('tr')[1:]):  # Skip header row\n",
    "        cols = row.find_all('td')\n",
    "        title_elem = row.find('th')  # Title is inside <th> instead of <td>\n",
    "        \n",
    "        if len(cols) < 4 or not title_elem:  # Ensure we have enough columns\n",
    "            continue\n",
    "        \n",
    "        rank = cols[0].text.strip()\n",
    "        peak = cols[1].text.strip()\n",
    "        title = title_elem.text.strip()\n",
    "        box_office = cols[2].text.strip()\n",
    "        year = cols[3].text.strip()\n",
    "\n",
    "        # Process Box Office: remove any letters before the first '$'\n",
    "        box_office = re.sub(r'^.*?(\\$)', r'\\1', box_office)\n",
    "\n",
    "        # Convert year to integer if possible\n",
    "        try:\n",
    "            year = int(year)\n",
    "        except ValueError:\n",
    "            year = None\n",
    "        \n",
    "        # Default values\n",
    "        country = \"Unknown\"\n",
    "        director = \"Unknown\"\n",
    "\n",
    "        # Visit the film page if a link is available\n",
    "        link_tag = title_elem.find('a')\n",
    "        film_url = BASE_URL + link_tag['href'] if link_tag else None\n",
    "        if film_url:\n",
    "            try:\n",
    "                film_response = requests.get(film_url)\n",
    "                film_soup = BeautifulSoup(film_response.text, 'html.parser')\n",
    "        \n",
    "                # Find the infobox (table with class 'infobox')\n",
    "                infobox = film_soup.find('table', {'class': 'infobox'})\n",
    "                if infobox:\n",
    "                    for row in infobox.find_all('tr'):\n",
    "                        header = row.find('th')\n",
    "                        if header:\n",
    "                            header_text = header.text.strip()\n",
    "                            content_cell = row.find('td')\n",
    "                            \n",
    "                            # Extract director(s)\n",
    "                            if \"Directed by\" in header_text and content_cell:\n",
    "                                director = extract_plainlist_content(content_cell)\n",
    "                                director = re.sub(r'\\[\\d+\\]', '', director)  # Remove citation numbers\n",
    "                                director = clean_joined_words(director)  # Apply additional cleaning\n",
    "                                \n",
    "                            # Extract country information\n",
    "                            elif ('Countries' in header_text or 'Country' in header_text) and content_cell:\n",
    "                                country = extract_plainlist_content(content_cell)\n",
    "                                country = re.sub(r'\\[\\d+\\]', '', country)  # Remove citation numbers\n",
    "                                country = clean_joined_words(country)  # Apply additional cleaning\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {title}: {e}\")\n",
    "            time.sleep(1)  # Prevent hitting Wikipedia too quickly\n",
    "\n",
    "        films.append({\n",
    "            \"rank\": rank, \n",
    "            \"peak\": peak, \n",
    "            \"title\": title, \n",
    "            \"box_office_revenue\": box_office,\n",
    "            \"release_year\": year,\n",
    "            \"country\": country,\n",
    "            \"director\": director\n",
    "        })\n",
    "\n",
    "    return films\n",
    "\n",
    "# Store films in the database\n",
    "def store_films_in_db(conn, cursor, films):\n",
    "    cursor.executemany(\"\"\"\n",
    "        INSERT INTO films (rank, peak, title, box_office_revenue, release_year, country, director)\n",
    "        VALUES (:rank, :peak, :title, :box_office_revenue, :release_year, :country, :director)\n",
    "    \"\"\", films)\n",
    "    conn.commit()\n",
    "\n",
    "# Read films from the database\n",
    "def read_films_from_db(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT rank, peak, title, box_office_revenue, release_year, country, director FROM films\")\n",
    "    \n",
    "    films = []\n",
    "    for row in cursor.fetchall():\n",
    "        films.append({\n",
    "            \"rank\": row[0],\n",
    "            \"peak\": row[1],\n",
    "            \"title\": row[2],\n",
    "            \"box_office_revenue\": row[3],\n",
    "            \"release_year\": row[4],\n",
    "            \"country\": row[5],\n",
    "            \"director\": row[6]\n",
    "        })\n",
    "    \n",
    "    return films\n",
    "\n",
    "# Save films to JSON\n",
    "def save_to_json(films, filename=\"temp/films.json\"):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(films, f, indent=4)\n",
    "        \n",
    "\n",
    "# Main execution\n",
    "print(\"Setting up database...\")\n",
    "conn, cursor = setup_database()\n",
    "\n",
    "print(\"Fetching film data from Wikipedia...\")\n",
    "films = fetch_film_data()\n",
    "\n",
    "print(\"Storing data in database...\")\n",
    "store_films_in_db(conn, cursor, films)\n",
    "\n",
    "print(\"Reading data from database...\")\n",
    "db_films = read_films_from_db(conn)\n",
    "\n",
    "print(\"Saving data to JSON file...\")\n",
    "save_to_json(db_films)\n",
    "\n",
    "print(\"Sample of parsed data:\")\n",
    "for film in db_films[:3]:\n",
    "    print(json.dumps(film, indent=2))\n",
    "\n",
    "conn.close()\n",
    "print(\"Process completed successfully!\")"
   ],
   "id": "253fab3d0e86f24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up database...\n",
      "Fetching film data from Wikipedia...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:22<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing data in database...\n",
      "Reading data from database...\n",
      "Saving data to JSON file...\n",
      "Sample of parsed data:\n",
      "{\n",
      "  \"rank\": \"1\",\n",
      "  \"peak\": \"1\",\n",
      "  \"title\": \"Avatar\",\n",
      "  \"box_office_revenue\": \"$2,923,706,026\",\n",
      "  \"release_year\": 2009,\n",
      "  \"country\": \"United Kingdom, United States\",\n",
      "  \"director\": \"James Cameron\"\n",
      "}\n",
      "{\n",
      "  \"rank\": \"2\",\n",
      "  \"peak\": \"1\",\n",
      "  \"title\": \"Avengers: Endgame\",\n",
      "  \"box_office_revenue\": \"$2,797,501,328\",\n",
      "  \"release_year\": 2019,\n",
      "  \"country\": \"United States\",\n",
      "  \"director\": \"Anthony Russo, Joe Russo\"\n",
      "}\n",
      "{\n",
      "  \"rank\": \"3\",\n",
      "  \"peak\": \"3\",\n",
      "  \"title\": \"Avatar: The Way of Water\",\n",
      "  \"box_office_revenue\": \"$2,320,250,281\",\n",
      "  \"release_year\": 2022,\n",
      "  \"country\": \"United States\",\n",
      "  \"director\": \"James Cameron\"\n",
      "}\n",
      "Process completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
